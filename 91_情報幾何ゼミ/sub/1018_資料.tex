\documentclass[report]{jlreq}
\usepackage{global}
\usepackage{./local}
\subfiletrue
\def\assetspath{../}
%\makeindex
\chead{2023/10/18}
\begin{document}

% ============================================================
%
% ============================================================

% ------------------------------------------------------------
%
% ------------------------------------------------------------
\section*{振り返りと導入}

前回は統計モデルを定義した。
本稿では次のことを行う:
\begin{itemize}
    \item 最尤推定量を定義する。
    \item Kullback-Leibler ダイバージェンスを定義する。
\end{itemize}

% ------------------------------------------------------------
%
% ------------------------------------------------------------
\section{最尤推定}

\begin{definition}[i.i.d.拡張]
    $\calX$上の統計モデル$(\Theta, \bfp)$に対し、
    $\calX^k$上の統計モデル$(\Theta, \bfp^k)$を
    $(\Theta, \bfp)$の
    \term{i.i.d.拡張}[i.i.d. extension]
        {i.i.d.拡張}[i.i.d.かくちょう]
    という。
    ただし
    $\bfp^k(\theta)
        \coloneqq
            \bfp(\theta) \times \cdots \times \bfp(\theta)$
    (積測度) である。
\end{definition}

\begin{propdef}[最尤推定量]
    可測写像$\hat{\theta} \colon \calX \to \Theta$に関する条件を考える。
    実現$(p, \mu)$に対し
    $\text{a.e.} x \in \calX, \;
        \hat{\theta}(x) \in \argmax_{\theta \in \Theta} p_\theta(x)$
    が成り立つとき、
    $\hat{\theta}$を
    \termsilent{$(p, \mu)$に関する最尤推定量}
    と呼ぶことにする。
    このとき次は同値である:
    \begin{enumerate}
        \item $\hat{\theta}$はある実現$(p, \mu)$に関する最尤推定量である。
        \item $\hat{\theta}$は任意の実現$(p, \mu)$に関する最尤推定量である。
    \end{enumerate}
    そこで、上の条件のいずれかが成り立つとき、
    $\hat{\theta}$を統計モデル$(\Theta, \bfp)$の
    \term{最尤推定量}[maximum likelihood estimator]
        {最尤推定量}[さいゆうすいていりょう]
    と呼ぶ。
\end{propdef}

\begin{proof}
    (2) $\Rightarrow$ (1) は明らか。
    (1) $\Rightarrow$ (2) は
    $\argmax$の定義および
    $q_\theta = p_\theta \dd[\mu]{\nu}$に注意すればわかる。
\end{proof}

\begin{proposition}[尤度方程式]
    (1) $\hat{\theta}$が最尤推定量
    ならば、
    (2) a.e.$x$に対し
    $\hat{\theta}(x)$は
    \term{尤度方程式}[likelihood equation]
        {尤度方程式}[ゆうどほうていしき]
    $(dl_x)_\theta = 0$の解である。
    さらに指数型分布族においては逆も成り立つ。
\end{proposition}

\begin{proof}
    (1) $\Rightarrow$ (2) は
    $l_x$が$\theta$に関し微分可能であることから従う。
    指数型分布族の場合、
    (2) $\Rightarrow$ (1) は
    $l_x(\theta) = \myangle{\theta}{T(x)} - \psi(\theta)$
    が上に凸であることから従う。
\end{proof}

\begin{example}[最尤推定量の例]
    ~
    \begin{itemize}
        \item 正規分布族の場合
        \item Cauchy 分布族の場合
    \end{itemize}
\end{example}

% ------------------------------------------------------------
%
% ------------------------------------------------------------
\section{Kullback-Leibler ダイバージェンス}

\begin{definition}[Kullback-Leibler ダイバージェンス]
    関数$D \colon \calP(\calX) \times \calP(\calX) \to [0, \infty],$
    \begin{equation}
        D(p \| q)
            \coloneqq
                \begin{cases}
                    E_q \mybracket{
                        \frac{dp}{dq}
                        \log \frac{dp}{dq}
                    }
                        =
                            E_p \mybracket{
                                \log \frac{dp}{dq}
                            }
                        & (p \ll q) \\
                    \infty
                        & (p \not\ll q)
                \end{cases}
    \end{equation}
    を$\calP(\calX)$上の
    \term{Kullback-Leibler ダイバージェンス}
        {Kullback-Leibler ダイバージェンス}[Kullback-Leibler ダイバージェンス]
    と呼ぶ。
\end{definition}

\begin{proposition}
    $\calP(\calX)$に
    全変動で定まる位相を入れると、
    KLダイバージェンスは連続とは限らない。
\end{proposition}

\begin{proof}
    $\calX \coloneqq \{ 0, 1 \}$として
    $p_n \coloneqq \frac{1}{n} \delta^0 + \myparen{1 - \frac{1}{n}} \delta^1, \;
        q_n \coloneqq e^{-n} \delta^0 + (1 - e^{-n}) \delta^1$
    が反例のひとつ。
\end{proof}

\begin{proposition}[指数型分布族とKLダイバージェンス]
    $\calP$を指数型分布族とする。
    最小次元実現$(V, T, \mu)$に対し
    対数分配関数$\psi$、自然パラメータ座標$\theta$、期待値パラメータ座標$\eta$を考える。
    このとき
    \begin{equation}
        D(p \| q)
            =
                \psi(\theta_q) + \psi^\vee(\eta_p)
                    - \myangle{\theta_q}{\eta_p}
                \quad
                (\forall p, q \in \calP)
    \end{equation}
    が成り立つ。
    ただし$\psi^\vee$は$\psi$の Legendre 変換である。
\end{proposition}

\begin{proof}
    Legendre 変換の定義より
    $\psi(\theta_p) + \psi^\vee(\eta_p)
        = \myangle{\theta_p}{\eta_p}$
    ゆえに
    \begin{alignat}{1}
        \psi(\theta_q) + \psi^\vee(\eta_p)
            - \myangle{\theta_q}{\eta_p}
            &=
                \psi(\theta_q) - \psi(\theta_p)
                    + \myangle{\theta_p}{\eta_p}
                    - \myangle{\theta_q}{\eta_p}
                \\
            &=
                E_p \mybracket{
                    \psi(\theta_q) - \psi(\theta_p)
                        + \myangle{\theta_p}{T}
                        - \myangle{\theta_q}{T}
                }
                \\
            &=
                E_p \mybracket{
                    \log \frac{dp}{dq}
                }
                \\
            &=
                D(p \| q)
    \end{alignat}
\end{proof}

$\calX = \{ 1, \ldots, n \}, n \in \N$ (カテゴリカル分布) の場合に
最尤推定量とKLダイバージェンスの関係を考える。

\begin{definition}[経験分布]
    $x = (x_1, \dots, x_k) \in \calX^k$に対し
    \begin{equation}
        \hat{p}_x
            \coloneqq
                \frac{1}{k}
                \sum_{i = 1}^k
                    \delta^{x_i}
    \end{equation}
    を$x$により定まる
    \term{経験分布}[empirical distribution]
        {経験分布}[けいけんぶんぷ]
    という。
\end{definition}

\begin{proposition}[最尤推定量とKLダイバージェンス]
    $(\Theta, \bfp)$を$\calX$上の統計モデルとし、
    $k$個のi.i.d.拡張$(\Theta, \bfp^k)$を考える。
    $x = (x_1, \dots, x_k) \in \calX^k$とし、
    $\hat{p}_x$を$x$により定まる経験分布とする。
    このとき、
    $\bfp^k(\Theta)$が
    $\hat{p}_x$を支配する確率測度を
    少なくともひとつ含むならば、
    次が成り立つ:
    \begin{equation}
        \argmin_{\theta \in \Theta} D(\hat{p}_x \| \bfp^k(\theta))
            =
                \argmax_{\theta \in \Theta} p_\theta^k(x)
    \end{equation}
\end{proposition}

\begin{proof}
    $\forall \theta \in \Theta$に対し
    \begin{alignat}{1}
        D(\hat{p}_x \| \bfp^k(\theta))
            &=
                E_{\hat{p}_x} \mybracket{
                    \log \frac{d\hat{p}_x}{d(\bfp^k(\theta))}
                } \\
            &=
                E_{\hat{p}_x} \mybracket{
                    \log \frac{d\hat{p}_x}{di}
                }
                -
                E_{\hat{p}_x} \mybracket{
                    \log \frac{d(\bfp^k(\theta))}{di}
                } \\
            &=
                (\text{$\theta$によらない項})
                -
                \frac{1}{k}
                \log p_\theta^k(x)
    \end{alignat}
    ゆえに命題の主張が従う。
\end{proof}


% ------------------------------------------------------------
%
% ------------------------------------------------------------
%\section{ダイバージェンスと双対構造}
%
%ダイバージェンスにより双対構造が誘導されることをみる。
%
%\begin{proposition}
%    $M$を多様体、
%    $D$を$M$上のダイバージェンスとする。
%    ダイバージェンスは
%    $M$上の双対構造を誘導する。
%\end{proposition}
%
%\begin{proof}
%    \begin{alignat}{1}
%        g_{ij}
%            &=
%                - D_{i; j}, \\
%        \Gamma_{ijk}
%            &=
%                - D_{ij; k}, \\
%        \Gamma^*_{ijk}
%            &=
%                - D_{k; ij}
%    \end{alignat}
%    と定めればよい。
%\end{proof}
%
%\begin{theorem}[一般化 Pythagoras の定理]
%    $P$と$Q$をつなぐ$\nabla^*$-測地線と
%    $Q$と$R$をつなぐ$\nabla$-測地線が
%    直交するとき、
%    次が成り立つ:
%    \begin{equation}
%        D(R \mid P)
%            =
%                D(Q \mid P)
%                +
%                D(R \mid Q)
%    \end{equation}
%\end{theorem}
%
%\begin{proof}
%    \TODO{}
%\end{proof}
%
%\begin{definition}[測地射影]
%    $\hat{P}_S$が$P$の$S$への\term{測地射影}[geodesic projection]
%        {測地射影}[そくちしゃえい]
%    であるとは、
%    $P$と$\hat{P}_S$をつなぐ$\nabla$-測地線が
%    $S$と直交することをいう。
%\end{definition}
%
%\begin{theorem}[射影定理]
%    $R$が$D[P \mid R], \; R \in S$
%    を最小化する点ならば、
%    $R$は$P$の$S$への双対測地射影
%    $\hat{P}^*_S$である。
%\end{theorem}
%
%\begin{proof}
%    一般化 Pythagoras の定理より
%    $D[P \mid Q] = D[P \mid \hat{P}^*_S] + D[\hat{P}^*_S \mid Q]$
%    であるから、
%    $Q = \hat{P}^*_S$は
%    $D[P \mid Q]$の臨界点である。
%\end{proof}
%
%\begin{theorem}
%    $S$が平坦ならば、
%    $P$の$S$への双対測地射影は一意的であり、
%    ダイバージェンスを最小化する点である。
%\end{theorem}
%
%\begin{proof}
%    primary および dual の一般化 Pythagoras の定理が成り立つことから従う。
%\end{proof}

% ------------------------------------------------------------
%
% ------------------------------------------------------------
\section*{今後の予定}

\begin{itemize}
    \item 射影
\end{itemize}

% ------------------------------------------------------------
%
% ------------------------------------------------------------
\section*{参考文献}

%Legendre 変換については
%\cite{niculescu_convex_2018}
%を参考にした。
%期待値パラメータに関しては
%\cite{wainwright_graphical_2007}を参考にした。

\nocite{amari_information_2016}
\nocite{amari_methods_2007}
\nocite{ay_information_2017}

{
    \renewcommand{\bibsection}{}
    \bibliographystyle{amsalpha}
    \bibliography{./bibliography,../../mybibliography}
}

% ------------------------------------------------------------
%
% ------------------------------------------------------------
%\newpage
%\appendix
%\renewcommand\thesection{\Alph{section}}
%\setcounter{section}{0}
%\section{付録}

\end{document}